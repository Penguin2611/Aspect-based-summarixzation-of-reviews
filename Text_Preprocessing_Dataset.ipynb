{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Preprocessing_Dataset.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Penguin2611/Aspect-based-summarization-of-reviews/blob/master/Text_Preprocessing_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abHJknWF_wK0",
        "colab_type": "code",
        "outputId": "306829cb-eef6-4942-f75a-6d0a19be0fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjxGpe4O-Xrz",
        "colab_type": "code",
        "outputId": "29f689e2-23c8-4546-cf8a-13b1c3fa2db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import string"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw9iqcBF--4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Tokenized Output\n",
        "## object creation\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# file = open('/content/drive/My Drive/amazon_review.txt')\n",
        "\n",
        "def preprocess(s):\n",
        "### Data Reading\n",
        "  # z = file.readlines()\n",
        "  # s = \"\"\n",
        "  # s = s.join(z)\n",
        "\n",
        "\n",
        "  ### Splitting input lines\n",
        "  ### Tokenizing line by line\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  l = WordNetLemmatizer()\n",
        "  t = TweetTokenizer() \n",
        "  for c in string.punctuation:   ### Removing Punctuations\n",
        "      if c == string.punctuation[13]:\n",
        "        s = s.replace(c,\" \")\n",
        "      if c != string.punctuation[6] and c != string.punctuation[12]:\n",
        "        s= s.replace(c,\"\")\n",
        "\n",
        "  res = s.splitlines()\n",
        "  print(res) \n",
        "  text = [] \n",
        "  for i in res:\n",
        "    p = t.tokenize(i) ### Tokenization\n",
        "    text.append(p)\n",
        "    text.append('\\n')\n",
        "\n",
        "  ## Flatting the list\n",
        "\n",
        "  flat_list = []\n",
        "  for sublist in text:\n",
        "      for item in sublist:\n",
        "          flat_list.append(item)\n",
        "\n",
        "  tokens = [token.lower() for token in flat_list]\n",
        "  tagged = nltk.pos_tag(tokens) ### POS Tagging \n",
        "\n",
        "\n",
        "  ### Combining list elements to string\n",
        "  ### Tokenized output\n",
        "\n",
        "  ### Lemmization\n",
        "  lem_text =[]\n",
        "  for c in tokens:\n",
        "    tokenize_text = l.lemmatize(c,pos='v')\n",
        "    tokenize_text = l.lemmatize(tokenize_text,pos='n')\n",
        "    tokenize_text = l.lemmatize(tokenize_text,pos='a')\n",
        "    lem_text.append(tokenize_text)\n",
        "\n",
        "  ### Stopwords Removal\n",
        "  stop_text = []\n",
        "  for w in lem_text: \n",
        "      if w not in stop_words: \n",
        "          stop_text.append(w)\n",
        "  print(\"Processed Tokens\",stop_text,'\\n')\n",
        "  \n",
        "\n",
        "  # s = \" \"\n",
        "  # output = s.join(stop_text)\n",
        "  # print(output)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0076_vSONxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "732f0d7a-8425-4961-b9f4-126f72e32d1d"
      },
      "source": [
        "# open input file: \n",
        "ifile = open('/content/drive/My Drive/Musical_Instruments_5.json') \n",
        "all_data = list()\n",
        "for i, line in enumerate(ifile): \n",
        "    # convert the json on this line to a dict\n",
        "    data = json.loads(line)\n",
        "    # extract what we want\n",
        "    text = data['reviewText']\n",
        "    prodid = data['asin']\n",
        "    # add to the data collected so far\n",
        "    all_data.append([prodid, text])\n",
        "# create the DataFrame\n",
        "df = pd.DataFrame(all_data, columns=['Product ID','Review'])\n",
        "## Group by product ID\n",
        "groupby_Id = df['Review'].groupby(df['Product ID'])\n",
        "## z = No. of products\n",
        "# z = len(list(groupby_Id))//100\n",
        "z = 1\n",
        "index = 0\n",
        "for j in range(z):\n",
        "  t = len(list(groupby_Id)[j][1])\n",
        "  # print(t)\n",
        "  for i in range(t):\n",
        "    ### Text Preprocessing of reviews for each Product\n",
        "    preprocess(list(groupby_Id)[j][1][index])\n",
        "    index += 1 \n",
        "ifile.close()\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Not much to write about here but it does exactly what it's supposed to  filters out the pop sounds  now my recordings are much more crisp  it is one of the lowest prices pop filters on amazon so might as well buy it they honestly work the same despite their pricing\"]\n",
            "Processed Tokens ['much', 'write', 'exactly', 'suppose', 'filter', 'pop', 'sound', 'record', 'much', 'crisp', 'one', 'low', 'price', 'pop', 'filter', 'amazon', 'might', 'well', 'buy', 'honestly', 'work', 'despite', 'price', '\\n'] \n",
            "\n",
            "[\"The product does exactly as it should and is quite affordable I did not realized it was double screened until it arrived so it was even better than I had expected As an added bonus one of the screens carries a small hint of the smell of an old grape candy I used to buy so for reminiscent's sake I cannot stop putting the pop filter next to my nose and smelling it after recording  DIf you needed a pop filter this will work just as well as the expensive ones and it may even come with a pleasing aroma like mine didBuy this product \"]\n",
            "Processed Tokens ['product', 'exactly', 'quite', 'affordable', 'realize', 'double', 'screen', 'arrive', 'even', 'good', 'expect', 'add', 'bonus', 'one', 'screen', 'carry', 'small', 'hint', 'smell', 'old', 'grape', 'candy', 'use', 'buy', \"reminiscent's\", 'sake', 'cannot', 'stop', 'put', 'pop', 'filter', 'next', 'nose', 'smell', 'record', 'dif', 'need', 'pop', 'filter', 'work', 'well', 'expensive', 'one', 'may', 'even', 'come', 'please', 'aroma', 'like', 'mine', 'didbuy', 'product', '\\n'] \n",
            "\n",
            "['The primary job of this device is to block the breath that would otherwise produce a popping sound while allowing your voice to pass through with no noticeable reduction of volume or high frequencies  The double cloth filter blocks the pops and lets the voice through with no coloration  The metal clamp mount attaches to the mike stand secure enough to keep it attached  The goose neck needs a little coaxing to stay where you put it ']\n",
            "Processed Tokens ['primary', 'job', 'device', 'block', 'breath', 'would', 'otherwise', 'produce', 'pop', 'sound', 'allow', 'voice', 'pas', 'noticeable', 'reduction', 'volume', 'high', 'frequency', 'double', 'cloth', 'filter', 'block', 'pop', 'let', 'voice', 'coloration', 'metal', 'clamp', 'mount', 'attach', 'mike', 'stand', 'secure', 'enough', 'keep', 'attach', 'goose', 'neck', 'need', 'little', 'coax', 'stay', 'put', '\\n'] \n",
            "\n",
            "['Nice windscreen protects my MXL mic and prevents pops  Only thing is that the gooseneck is only marginally able to hold the screen in position and requires careful positioning of the clamp to avoid sagging ']\n",
            "Processed Tokens ['nice', 'windscreen', 'protect', 'mxl', 'mic', 'prevent', 'pop', 'thing', 'gooseneck', 'marginally', 'able', 'hold', 'screen', 'position', 'require', 'careful', 'position', 'clamp', 'avoid', 'sag', '\\n'] \n",
            "\n",
            "[\"This pop filter is great  It looks and performs like a studio filter  If you're recording vocals this will eliminate the pops that gets recorded when you sing \"]\n",
            "Processed Tokens ['pop', 'filter', 'great', 'look', 'perform', 'like', 'studio', 'filter', 'record', 'vocal', 'eliminate', 'pop', 'get', 'record', 'sing', '\\n'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}