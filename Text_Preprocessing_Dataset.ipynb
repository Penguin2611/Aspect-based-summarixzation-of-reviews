{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Penguin2611/Aspect-based-summarization-of-reviews/blob/master/Text_Preprocessing_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "abHJknWF_wK0",
    "outputId": "306829cb-eef6-4942-f75a-6d0a19be0fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "tjxGpe4O-Xrz",
    "outputId": "29f689e2-23c8-4546-cf8a-13b1c3fa2db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tushar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Tushar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tushar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sw9iqcBF--4u"
   },
   "outputs": [],
   "source": [
    "## Tokenized Output\n",
    "## object creation\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# file = open('/content/drive/My Drive/amazon_review.txt')\n",
    "\n",
    "def preprocess(s):\n",
    "### Data Reading\n",
    "  # z = file.readlines()\n",
    "  # s = \"\"\n",
    "  # s = s.join(z)\n",
    "\n",
    "\n",
    "  ### Splitting input lines\n",
    "  ### Tokenizing line by line\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  l = WordNetLemmatizer()\n",
    "  t = TweetTokenizer() \n",
    "  for c in string.punctuation:   ### Removing Punctuations\n",
    "      if c == string.punctuation[13]:\n",
    "        s = s.replace(c,\" \")\n",
    "      if c != string.punctuation[6] and c != string.punctuation[12]:\n",
    "        s= s.replace(c,\"\")\n",
    "\n",
    "  res = s.splitlines()\n",
    "  print(res) \n",
    "  text = [] \n",
    "  for i in res:\n",
    "    p = t.tokenize(i) ### Tokenization\n",
    "    text.append(p)\n",
    "    text.append('\\n')\n",
    "\n",
    "  ## Flatting the list\n",
    "\n",
    "  flat_list = []\n",
    "  for sublist in text:\n",
    "      for item in sublist:\n",
    "          flat_list.append(item)\n",
    "\n",
    "  tokens = [token.lower() for token in flat_list]\n",
    "  tagged = nltk.pos_tag(tokens) ### POS Tagging \n",
    "\n",
    "\n",
    "  ### Combining list elements to string\n",
    "  ### Tokenized output\n",
    "\n",
    "  ### Lemmization\n",
    "  lem_text =[]\n",
    "  for c in tokens:\n",
    "    tokenize_text = l.lemmatize(c,pos='v')\n",
    "    tokenize_text = l.lemmatize(tokenize_text,pos='n')\n",
    "    tokenize_text = l.lemmatize(tokenize_text,pos='a')\n",
    "    lem_text.append(tokenize_text)\n",
    "  \n",
    "  tagged = nltk.pos_tag(lem_text)\n",
    "  ### Stopwords Removal\n",
    "  stop_text = []\n",
    "  for w in lem_text: \n",
    "      if w not in stop_words: \n",
    "          stop_text.append(w)\n",
    "  print(\"Processed Tokens\",stop_text,'\\n')\n",
    "  print(\"POS tagging:\",tagged,'\\n')\n",
    "\n",
    "  # s = \" \"\n",
    "  # output = s.join(stop_text)\n",
    "  # print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "R0076_vSONxT",
    "outputId": "732f0d7a-8425-4961-b9f4-126f72e32d1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Not much to write about here but it does exactly what it's supposed to  filters out the pop sounds  now my recordings are much more crisp  it is one of the lowest prices pop filters on amazon so might as well buy it they honestly work the same despite their pricing\"]\n",
      "Processed Tokens ['much', 'write', 'exactly', 'suppose', 'filter', 'pop', 'sound', 'record', 'much', 'crisp', 'one', 'low', 'price', 'pop', 'filter', 'amazon', 'might', 'well', 'buy', 'honestly', 'work', 'despite', 'price', '\\n'] \n",
      "\n",
      "POS tagging: [('not', 'RB'), ('much', 'JJ'), ('to', 'TO'), ('write', 'VB'), ('about', 'IN'), ('here', 'RB'), ('but', 'CC'), ('it', 'PRP'), ('do', 'VBP'), ('exactly', 'RB'), ('what', 'WP'), (\"it's\", 'VBZ'), ('suppose', 'NN'), ('to', 'TO'), ('filter', 'VB'), ('out', 'RP'), ('the', 'DT'), ('pop', 'NN'), ('sound', 'NN'), ('now', 'RB'), ('my', 'PRP$'), ('record', 'NN'), ('be', 'VB'), ('much', 'RB'), ('more', 'JJR'), ('crisp', 'NN'), ('it', 'PRP'), ('be', 'VB'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('low', 'JJ'), ('price', 'NN'), ('pop', 'NN'), ('filter', 'NN'), ('on', 'IN'), ('amazon', 'NNS'), ('so', 'RB'), ('might', 'MD'), ('a', 'DT'), ('well', 'RB'), ('buy', 'VB'), ('it', 'PRP'), ('they', 'PRP'), ('honestly', 'RB'), ('work', 'VBP'), ('the', 'DT'), ('same', 'JJ'), ('despite', 'IN'), ('their', 'PRP$'), ('price', 'NN'), ('\\n', 'NN')] \n",
      "\n",
      "[\"The product does exactly as it should and is quite affordable I did not realized it was double screened until it arrived so it was even better than I had expected As an added bonus one of the screens carries a small hint of the smell of an old grape candy I used to buy so for reminiscent's sake I cannot stop putting the pop filter next to my nose and smelling it after recording  DIf you needed a pop filter this will work just as well as the expensive ones and it may even come with a pleasing aroma like mine didBuy this product \"]\n",
      "Processed Tokens ['product', 'exactly', 'quite', 'affordable', 'realize', 'double', 'screen', 'arrive', 'even', 'good', 'expect', 'add', 'bonus', 'one', 'screen', 'carry', 'small', 'hint', 'smell', 'old', 'grape', 'candy', 'use', 'buy', \"reminiscent's\", 'sake', 'cannot', 'stop', 'put', 'pop', 'filter', 'next', 'nose', 'smell', 'record', 'dif', 'need', 'pop', 'filter', 'work', 'well', 'expensive', 'one', 'may', 'even', 'come', 'please', 'aroma', 'like', 'mine', 'didbuy', 'product', '\\n'] \n",
      "\n",
      "POS tagging: [('the', 'DT'), ('product', 'NN'), ('do', 'VB'), ('exactly', 'RB'), ('a', 'DT'), ('it', 'PRP'), ('should', 'MD'), ('and', 'CC'), ('be', 'VB'), ('quite', 'RB'), ('affordable', 'JJ'), ('i', 'NNS'), ('do', 'VBP'), ('not', 'RB'), ('realize', 'VB'), ('it', 'PRP'), ('be', 'VB'), ('double', 'JJ'), ('screen', 'NN'), ('until', 'IN'), ('it', 'PRP'), ('arrive', 'VBZ'), ('so', 'IN'), ('it', 'PRP'), ('be', 'VB'), ('even', 'RB'), ('good', 'JJ'), ('than', 'IN'), ('i', 'NNS'), ('have', 'VBP'), ('expect', 'VBP'), ('a', 'DT'), ('an', 'DT'), ('add', 'JJ'), ('bonus', 'NN'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('screen', 'NN'), ('carry', 'VBP'), ('a', 'DT'), ('small', 'JJ'), ('hint', 'NN'), ('of', 'IN'), ('the', 'DT'), ('smell', 'NN'), ('of', 'IN'), ('an', 'DT'), ('old', 'JJ'), ('grape', 'NN'), ('candy', 'NN'), ('i', 'NN'), ('use', 'VBP'), ('to', 'TO'), ('buy', 'VB'), ('so', 'RB'), ('for', 'IN'), (\"reminiscent's\", 'JJ'), ('sake', 'NN'), ('i', 'NN'), ('cannot', 'VBP'), ('stop', 'VB'), ('put', 'VBD'), ('the', 'DT'), ('pop', 'NN'), ('filter', 'NN'), ('next', 'IN'), ('to', 'TO'), ('my', 'PRP$'), ('nose', 'JJ'), ('and', 'CC'), ('smell', 'NN'), ('it', 'PRP'), ('after', 'IN'), ('record', 'NN'), ('dif', 'NN'), ('you', 'PRP'), ('need', 'VBP'), ('a', 'DT'), ('pop', 'NN'), ('filter', 'NN'), ('this', 'DT'), ('will', 'MD'), ('work', 'VB'), ('just', 'RB'), ('a', 'DT'), ('well', 'RB'), ('a', 'DT'), ('the', 'DT'), ('expensive', 'JJ'), ('one', 'CD'), ('and', 'CC'), ('it', 'PRP'), ('may', 'MD'), ('even', 'RB'), ('come', 'VB'), ('with', 'IN'), ('a', 'DT'), ('please', 'NN'), ('aroma', 'NN'), ('like', 'IN'), ('mine', 'JJ'), ('didbuy', 'NN'), ('this', 'DT'), ('product', 'NN'), ('\\n', 'NN')] \n",
      "\n",
      "['The primary job of this device is to block the breath that would otherwise produce a popping sound while allowing your voice to pass through with no noticeable reduction of volume or high frequencies  The double cloth filter blocks the pops and lets the voice through with no coloration  The metal clamp mount attaches to the mike stand secure enough to keep it attached  The goose neck needs a little coaxing to stay where you put it ']\n",
      "Processed Tokens ['primary', 'job', 'device', 'block', 'breath', 'would', 'otherwise', 'produce', 'pop', 'sound', 'allow', 'voice', 'pas', 'noticeable', 'reduction', 'volume', 'high', 'frequency', 'double', 'cloth', 'filter', 'block', 'pop', 'let', 'voice', 'coloration', 'metal', 'clamp', 'mount', 'attach', 'mike', 'stand', 'secure', 'enough', 'keep', 'attach', 'goose', 'neck', 'need', 'little', 'coax', 'stay', 'put', '\\n'] \n",
      "\n",
      "POS tagging: [('the', 'DT'), ('primary', 'JJ'), ('job', 'NN'), ('of', 'IN'), ('this', 'DT'), ('device', 'NN'), ('be', 'VB'), ('to', 'TO'), ('block', 'VB'), ('the', 'DT'), ('breath', 'NN'), ('that', 'WDT'), ('would', 'MD'), ('otherwise', 'VB'), ('produce', 'VB'), ('a', 'DT'), ('pop', 'NN'), ('sound', 'NN'), ('while', 'IN'), ('allow', 'VB'), ('your', 'PRP$'), ('voice', 'NN'), ('to', 'TO'), ('pas', 'VB'), ('through', 'IN'), ('with', 'IN'), ('no', 'DT'), ('noticeable', 'JJ'), ('reduction', 'NN'), ('of', 'IN'), ('volume', 'NN'), ('or', 'CC'), ('high', 'JJ'), ('frequency', 'NN'), ('the', 'DT'), ('double', 'JJ'), ('cloth', 'NN'), ('filter', 'NN'), ('block', 'NN'), ('the', 'DT'), ('pop', 'NN'), ('and', 'CC'), ('let', 'VB'), ('the', 'DT'), ('voice', 'NN'), ('through', 'IN'), ('with', 'IN'), ('no', 'DT'), ('coloration', 'NN'), ('the', 'DT'), ('metal', 'NN'), ('clamp', 'NN'), ('mount', 'NN'), ('attach', 'NN'), ('to', 'TO'), ('the', 'DT'), ('mike', 'NN'), ('stand', 'VBP'), ('secure', 'NN'), ('enough', 'NN'), ('to', 'TO'), ('keep', 'VB'), ('it', 'PRP'), ('attach', 'RP'), ('the', 'DT'), ('goose', 'NN'), ('neck', 'NN'), ('need', 'VBP'), ('a', 'DT'), ('little', 'JJ'), ('coax', 'NN'), ('to', 'TO'), ('stay', 'VB'), ('where', 'WRB'), ('you', 'PRP'), ('put', 'VBD'), ('it', 'PRP'), ('\\n', 'RP')] \n",
      "\n",
      "['Nice windscreen protects my MXL mic and prevents pops  Only thing is that the gooseneck is only marginally able to hold the screen in position and requires careful positioning of the clamp to avoid sagging ']\n",
      "Processed Tokens ['nice', 'windscreen', 'protect', 'mxl', 'mic', 'prevent', 'pop', 'thing', 'gooseneck', 'marginally', 'able', 'hold', 'screen', 'position', 'require', 'careful', 'position', 'clamp', 'avoid', 'sag', '\\n'] \n",
      "\n",
      "POS tagging: [('nice', 'JJ'), ('windscreen', 'NN'), ('protect', 'VBP'), ('my', 'PRP$'), ('mxl', 'JJ'), ('mic', 'JJ'), ('and', 'CC'), ('prevent', 'JJ'), ('pop', 'NN'), ('only', 'JJ'), ('thing', 'NN'), ('be', 'VB'), ('that', 'IN'), ('the', 'DT'), ('gooseneck', 'NN'), ('be', 'VB'), ('only', 'RB'), ('marginally', 'RB'), ('able', 'JJ'), ('to', 'TO'), ('hold', 'VB'), ('the', 'DT'), ('screen', 'NN'), ('in', 'IN'), ('position', 'NN'), ('and', 'CC'), ('require', 'VB'), ('careful', 'JJ'), ('position', 'NN'), ('of', 'IN'), ('the', 'DT'), ('clamp', 'NN'), ('to', 'TO'), ('avoid', 'VB'), ('sag', 'NN'), ('\\n', 'NN')] \n",
      "\n",
      "[\"This pop filter is great  It looks and performs like a studio filter  If you're recording vocals this will eliminate the pops that gets recorded when you sing \"]\n",
      "Processed Tokens ['pop', 'filter', 'great', 'look', 'perform', 'like', 'studio', 'filter', 'record', 'vocal', 'eliminate', 'pop', 'get', 'record', 'sing', '\\n'] \n",
      "\n",
      "POS tagging: [('this', 'DT'), ('pop', 'NN'), ('filter', 'NN'), ('be', 'VB'), ('great', 'JJ'), ('it', 'PRP'), ('look', 'VB'), ('and', 'CC'), ('perform', 'VB'), ('like', 'IN'), ('a', 'DT'), ('studio', 'NN'), ('filter', 'NN'), ('if', 'IN'), (\"you're\", 'NN'), ('record', 'NN'), ('vocal', 'NN'), ('this', 'DT'), ('will', 'MD'), ('eliminate', 'VB'), ('the', 'DT'), ('pop', 'NN'), ('that', 'WDT'), ('get', 'NN'), ('record', 'NN'), ('when', 'WRB'), ('you', 'PRP'), ('sing', 'VBP'), ('\\n', 'JJ')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# open input file: \n",
    "ifile = open('Musical_Instruments_5.json') \n",
    "all_data = list()\n",
    "for i, line in enumerate(ifile): \n",
    "    # convert the json on this line to a dict\n",
    "    data = json.loads(line)\n",
    "    # extract what we want\n",
    "    text = data['reviewText']\n",
    "    prodid = data['asin']\n",
    "    # add to the data collected so far\n",
    "    all_data.append([prodid, text])\n",
    "# create the DataFrame\n",
    "df = pd.DataFrame(all_data, columns=['Product ID','Review'])\n",
    "## Group by product ID\n",
    "groupby_Id = df['Review'].groupby(df['Product ID'])\n",
    "## z = No. of products\n",
    "# z = len(list(groupby_Id))//100\n",
    "z = 1\n",
    "index = 0\n",
    "for j in range(z):\n",
    "  t = len(list(groupby_Id)[j][1])\n",
    "  # print(t)\n",
    "  for i in range(t):\n",
    "    ### Text Preprocessing of reviews for each Product\n",
    "    preprocess(list(groupby_Id)[j][1][index])\n",
    "    index += 1 \n",
    "ifile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2e29360d904e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0min_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# frogged json sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0min_subj_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# from duoman subjectivity lexicon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mout_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# [describe output]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mjson_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "in_file = sys.argv[1] # frogged json sentences\n",
    "in_subj_list = sys.argv[2] # from duoman subjectivity lexicon\n",
    "out_file = sys.argv[3] # [describe output]\n",
    "json_out = sys.argv[4]\n",
    "\n",
    "############## Helpers ###############\n",
    "\n",
    "def extract_pos(word):\n",
    "    return word['pos'].split('(')[0]\n",
    "\n",
    "def check_subjectivity(phrase, subj):\n",
    "    subjective = list(set([word['word'].lower() for word in phrase if word['word'].lower() in subj] + [word['lemma'].lower() for word in phrase if word['lemma'].lower() in subj]))\n",
    "    return subjective\n",
    "\n",
    "def score_polarity(subjective_word,subj_pol):\n",
    "    try:\n",
    "        pol = subj_pol[subjective_word]\n",
    "    except:\n",
    "        pol = '+/-'\n",
    "    return pol\n",
    "\n",
    "def assess_phrase(phrase, subj, subj_pol):\n",
    "    if len(phrase) == 1: # no phrase\n",
    "        return False\n",
    "    poss = [extract_pos(w) for w in phrase]\n",
    "    if poss.count('ADJ') == 0: # pro or con should have at least one adjective\n",
    "        return False\n",
    "    if extract_pos(phrase[-1]) == 'VZ': # pro or con can not end with preposition\n",
    "        return False\n",
    "    if extract_pos(phrase[-1]) == 'WW': # pro or con can not end with particular variant of verb\n",
    "        if phrase[-1]['pos'][:5] == 'WW(pv':\n",
    "            return False\n",
    "    if extract_pos(phrase[0]) == 'WW': # pro or con can not start with particular variant of verb\n",
    "        if phrase[0]['pos'][:5] == 'WW(pv':\n",
    "            return False\n",
    "    if extract_pos(phrase[-1]) == 'VG': # pro or con can not end with conjunction\n",
    "        return False\n",
    "    subjective_words = check_subjectivity(phrase,subj)\n",
    "    if len(subjective_words) > 0: # pro or con should contain a subjective word\n",
    "        # decide polarity (pro or con)\n",
    "        polarities = [score_polarity(sw,subj_pol) for sw in subjective_words]\n",
    "        if len(list(set(polarities))) == 1:\n",
    "            polarity = polarities[0]\n",
    "        else:\n",
    "            sorted_polarity_counts = sorted([[x,polarities.count(x)] for x in list(set(polarities))],key = lambda k : k[1],reverse=True)\n",
    "            if sorted_polarity_counts[0][1] > sorted_polarity_counts[1][1]:\n",
    "                polarity = sorted_polarity_counts[0][0]\n",
    "            else:\n",
    "                if len(sorted_polarity_counts) == 2:\n",
    "                    if '+/-' in polarities:\n",
    "                        polarity = [x for x in polarities if x != '+/-'][0]\n",
    "                    else: # '+' and '-'\n",
    "                        polarity = '-'\n",
    "                else: # all three are in there\n",
    "                    if sorted_polarity_counts[0][1] > sorted_polarity_counts[2][1]:\n",
    "                        if '+/-' in [x[0] for x in sorted_polarity_counts[:2]]:\n",
    "                            polarity = [x[0] for x in sorted_polarity_counts[:2] if x[0] != '+/-'][0]\n",
    "                        else: # '+' and '-'\n",
    "                            polarity = '-'\n",
    "                    else: # same number of words with polarity\n",
    "                        polarity = '-'\n",
    "        return([subjective_words,polarities,polarity]) # return information\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "############## Preparations ###############\n",
    "\n",
    "# read in subjectivity lexicon\n",
    "subj_polarity = {}\n",
    "with open(in_subj_list,'r',encoding='utf-8') as file_in:\n",
    "    subj_assessments = file_in.read().strip().split('\\n')\n",
    "\n",
    "# generate subjective word-polarity dictionary\n",
    "for line in subj_assessments:\n",
    "    tokens = line.split('\\t')\n",
    "    word_pos = tokens[0].split()\n",
    "    word = word_pos[0]\n",
    "    pos = word_pos[1]\n",
    "    assessments = tokens[1:]\n",
    "    polarity = list(set([assessment[0] for assessment in assessments]))\n",
    "    if len(polarity) == 1:\n",
    "        pol = polarity[0]\n",
    "    else:\n",
    "        if '-' in polarity and '+' in polarity:\n",
    "            # check if one of the two is given double\n",
    "            if '++' in assessments and '-' in assessments:\n",
    "                pol = '+'\n",
    "            elif '--' in assessments and '+' in assessments:\n",
    "                pol = '-'\n",
    "            else:\n",
    "                continue\n",
    "    subj_polarity[word] = pol\n",
    "subjectivities = subj_polarity.keys()\n",
    "\n",
    "############## Main script ###############\n",
    "\n",
    "# initiate lists to write the output to\n",
    "output = []\n",
    "output_full = []\n",
    "\n",
    "# read in reviews (as frogged sentences in json format)\n",
    "with open(infile,'r',encoding='utf-8') as file_in:\n",
    "    reviews = json.loads(file_in.read())\n",
    "\n",
    "matches = [] # to save all matching patterns for this review\n",
    "for review in reviews: # for each review\n",
    "    for sentence in review: # for each sentence\n",
    "        ph = False # to check whether a phrase matching a pattern is being parsed\n",
    "        review_index = sentence[0]['review_id']\n",
    "        sentext = ' '.join([token['word'] for token in sentence])\n",
    "        for i,word in enumerate(sentence): # for each word in the sentence\n",
    "            if extract_pos(word) == 'ADJ': # might match pattern for subject phrase\n",
    "                if not ph: # start phrase\n",
    "                    phrase = [word]\n",
    "                    ph = True\n",
    "                else:\n",
    "                    if extract_pos(phrase[-1]) == 'N' or extract_pos(phrase[-1]) == 'ADJ' or phrase[-1]['phrase'][1:] == '-VP' or phrase[-1]['phrase'] == 'B-NP' or extract_pos(phrase[-1]) == 'VZ': # ADJ can be added to phrase\n",
    "                        phrase.append(word)\n",
    "                    else:\n",
    "                        polarity = assess_phrase(phrase,subjectivities,subj_polarity)\n",
    "                        if polarity:\n",
    "                            matches.append([review_index,sentext,' '.join([x['word'] for x in phrase]),' '.join([extract_pos(x) for x in phrase]),' '.join([x['phrase'] for x in phrase])] + polarity)\n",
    "                        phrase = [word]\n",
    "            elif extract_pos(word) == 'N': # might match pattern for subject phrase\n",
    "                if not ph: # start phrase\n",
    "                    phrase = [word]\n",
    "                    ph = True\n",
    "                else:\n",
    "                    if extract_pos(phrase[-1]) == 'N' or extract_pos(phrase[-1]) == 'ADJ' or extract_pos(phrase[-1]) == 'VZ' or phrase[-1]['phrase'] == 'B-NP' or phrase[-1]['phrase'] == 'I-ADJP': # N can be added to phrase\n",
    "                        phrase.append(word)\n",
    "                    else:\n",
    "                        polarity = assess_phrase(phrase,subjectivities,subj_polarity)\n",
    "                        if polarity:\n",
    "                            matches.append([review_index,sentext,' '.join([x['word'] for x in phrase]),' '.join([extract_pos(x) for x in phrase]),' '.join([x['phrase'] for x in phrase])] + polarity)\n",
    "                        phrase = [word]               \n",
    "            elif word['phrase'] == 'B-ADVP':\n",
    "                if not ph:\n",
    "                    phrase = [word]\n",
    "                    ph = True\n",
    "                else:\n",
    "                    polarity = assess_phrase(phrase,subjectivities,subj_polarity)\n",
    "                    if polarity:\n",
    "                        matches.append([review_index,sentext,' '.join([x['word'] for x in phrase]),' '.join([extract_pos(x) for x in phrase]),' '.join([x['phrase'] for x in phrase])] + polarity)\n",
    "                    phrase = [word]                    \n",
    "            elif word['phrase'] == 'B-NP':\n",
    "                if not ph:\n",
    "                    phrase = [word]\n",
    "                    ph = True\n",
    "                else:\n",
    "                    if extract_pos(phrase[-1]) == 'VZ' or phrase[-1]['phrase'] == 'B-ADVP':\n",
    "                        phrase.append(word)\n",
    "                    else:\n",
    "                        polarity = assess_phrase(phrase,subjectivities,subj_polarity)\n",
    "                        if polarity:\n",
    "                            matches.append([review_index,sentext,' '.join([x['word'] for x in phrase]),' '.join([extract_pos(x) for x in phrase]),' '.join([x['phrase'] for x in phrase])] + polarity)\n",
    "                        phrase = [word]                    \n",
    "            elif word['phrase'] == 'I-NP':\n",
    "                if ph:\n",
    "                    phrase.append(word)\n",
    "            elif word['phrase'] == 'I-ADJP':\n",
    "                if ph:\n",
    "                    if extract_pos(phrase[-1]) == 'ADJ':\n",
    "                        phrase.append(word)\n",
    "                    else:\n",
    "                        polarity = assess_phrase(phrase,subjectivities,subj_polarity)\n",
    "                        if polarity:\n",
    "                            matches.append([review_index,sentext,' '.join([x['word'] for x in phrase]),' '.join([extract_pos(x) for x in phrase]),' '.join([x['phrase'] for x in phrase])] + polarity)\n",
    "                        ph = False                       \n",
    "                        phrase = []\n",
    "            elif extract_pos(word) == 'VZ': # might continue pattern for subject phrase\n",
    "                if ph:\n",
    "                    if extract_pos(phrase[-1]) == 'ADJ': # fits pattern\n",
    "                        phrase.append(word)\n",
    "                    else:\n",
    "                        polarity = assess_phrase(phrase,subjectivities,subj_polarity)\n",
    "                        if polarity:\n",
    "                            matches.append([review_index,sentext,' '.join([x['word'] for x in phrase]),' '.join([extract_pos(x) for x in phrase]),' '.join([x['phrase'] for x in phrase])] + polarity)\n",
    "                        ph = False                       \n",
    "                        phrase = []\n",
    "                else:\n",
    "                    phrase = [word]\n",
    "                    ph = True\n",
    "            elif word['phrase'][1:] == '-VP': # might continue pattern for subject phrase\n",
    "                if word['phrase'] == 'B-VP':\n",
    "                    if not ph:\n",
    "                        phrase = [word]\n",
    "                        ph = True\n",
    "                    else:\n",
    "                        if extract_pos(phrase[-1]) == 'ADJ' or phrase[-1]['phrase'] == 'B-NP' or extract_pos(phrase[-1]) == 'N' or phrase[-1]['phrase'] == 'B-ADVP': # fits pattern\n",
    "                            phrase.append(word)\n",
    "                        else:\n",
    "                            polarity = assess_phrase(phrase,subjectivities,subj_polarity)\n",
    "                            if polarity:\n",
    "                                matches.append([review_index,sentext,' '.join([x['word'] for x in phrase]),' '.join([extract_pos(x) for x in phrase]),' '.join([x['phrase'] for x in phrase])] + polarity)\n",
    "                            ph = False\n",
    "                            phrase = []\n",
    "                else: \n",
    "                    if ph:\n",
    "                        if phrase[-1]['phrase'][1:] == '-VP':\n",
    "                            phrase.append(word)\n",
    "                        else:\n",
    "                            polarity = assess_phrase(phrase,subjectivities,subj_polarity)\n",
    "                            if polarity:\n",
    "                                matches.append([review_index,sentext,' '.join([x['word'] for x in phrase]),' '.join([extract_pos(x) for x in phrase]),' '.join([x['phrase'] for x in phrase])] + polarity)\n",
    "                            ph = False\n",
    "                            phrase = []\n",
    "            else:\n",
    "                if ph:\n",
    "                    polarity = assess_phrase(phrase,subjectivities,subj_polarity)\n",
    "                    if polarity:\n",
    "                        matches.append([review_index,sentext,' '.join([x['word'] for x in phrase]),' '.join([extract_pos(x) for x in phrase]),' '.join([x['phrase'] for x in phrase])] + polarity)\n",
    "                    ph = False\n",
    "                    phrase = []\n",
    "                else:\n",
    "                    phrase = []\n",
    "\n",
    "############## Write output ###############            \n",
    "\n",
    "# Simple overview of output as tab-separated data\n",
    "with open(outfile,'w',encoding='utf-8') as out:\n",
    "    out.write('\\n'.join(['\\t'.join([str(token) for token in match]) for match in matches]))\n",
    "\n",
    "# Full overview in json\n",
    "review_id_out = {}\n",
    "for match in matches:\n",
    "    review_id = str(match[0])\n",
    "    if review_id not in review_id_out.keys():\n",
    "        review_id_out[review_id] = {'text':match[1],'pattern_pros':[],'pattern_cons':[]}\n",
    "    if match[-1] == '+':\n",
    "        review_id_out[review_id]['pattern_pros'].append(match[2])\n",
    "    else:\n",
    "        review_id_out[review_id]['pattern_cons'].append(match[2])\n",
    "patterns_json_out = []\n",
    "for review_id in review_id_out.keys():\n",
    "    review_dict = review_id_out[review_id]\n",
    "    review_dict['index'] = review_id\n",
    "    patterns_json_out.append(review_dict)\n",
    "\n",
    "with open(json_out,'w',encoding='utf-8') as out:\n",
    "    json.dump(patterns_json_out,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Text_Preprocessing_Dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
