{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Preprocessing_Dataset.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Penguin2611/Aspect-based-summarization-of-reviews/blob/master/Text_Preprocessing_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abHJknWF_wK0",
        "colab_type": "code",
        "outputId": "cce4c4ef-5422-407a-9c19-3f86da787e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=False)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjxGpe4O-Xrz",
        "colab_type": "code",
        "outputId": "55a18373-85b2-4d01-ac65-c8a95d148ee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import string\n",
        "import gensim \n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw9iqcBF--4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Tokenized Output\n",
        "## object creation\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# file = open('/content/drive/My Drive/amazon_review.txt')\n",
        "\n",
        "def preprocess(s):\n",
        "### Data Reading\n",
        "  # z = file.readlines()\n",
        "  # s = \"\"\n",
        "  # s = s.join(z)\n",
        "\n",
        "\n",
        "  ### Splitting input lines\n",
        "  ### Tokenizing line by line\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  l = WordNetLemmatizer()\n",
        "  t = TweetTokenizer() \n",
        "  for c in string.punctuation:   ### Removing Punctuations\n",
        "      if c == string.punctuation[13]:\n",
        "        s = s.replace(c,\" \")\n",
        "      if c != string.punctuation[6] and c != string.punctuation[12]:\n",
        "        s= s.replace(c,\"\")\n",
        "\n",
        "  res = s.splitlines()\n",
        "  print(res) \n",
        "  text = [] \n",
        "  for i in res:\n",
        "    p = t.tokenize(i) ### Tokenization\n",
        "    text.append(p)\n",
        "    text.append('\\n')\n",
        "\n",
        "  ## Flatting the list\n",
        "\n",
        "  flat_list = []\n",
        "  for sublist in text:\n",
        "      for item in sublist:\n",
        "          flat_list.append(item)\n",
        "\n",
        "  tokens = [token.lower() for token in flat_list]\n",
        " \n",
        "\n",
        "\n",
        "  ### Combining list elements to string\n",
        "  ### Tokenized output\n",
        "\n",
        "  ### Lemmization\n",
        "  lem_text =[]\n",
        "  for c in tokens:\n",
        "    tokenize_text = l.lemmatize(c,pos='v')\n",
        "    tokenize_text = l.lemmatize(tokenize_text,pos='n')\n",
        "    tokenize_text = l.lemmatize(tokenize_text,pos='a')\n",
        "    lem_text.append(tokenize_text)\n",
        "\n",
        "  ### Stopwords Removal\n",
        "  stop_text = []\n",
        "  for w in lem_text: \n",
        "      if w not in stop_words: \n",
        "          stop_text.append(w)\n",
        "  # print(\"Processed Tokens\",stop_text)\n",
        "  tagged = nltk.pos_tag(stop_text) ### POS Tagging \n",
        "  return tagged\n",
        "\n",
        "  # s = \" \"\n",
        "  # output = s.join(stop_text)\n",
        "  # print(output)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxU6CSz3_eE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Word embeddings\n",
        "def word_embedding(data):\n",
        "  model = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5)\n",
        "  print(model.similarity(data,'good'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0076_vSONxT",
        "colab_type": "code",
        "outputId": "81d99842-4dcb-4023-f5b2-b49bb4f2c88e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# open input file: \n",
        "ifile = open('/content/drive/My Drive/Musical_Instruments_5.json') \n",
        "all_data = list()\n",
        "for i, line in enumerate(ifile): \n",
        "    # convert the json on this line to a dict\n",
        "    data = json.loads(line)\n",
        "    # extract what we want\n",
        "    text = data['reviewText']\n",
        "    prodid = data['asin']\n",
        "    # add to the data collected so far\n",
        "    all_data.append([prodid, text])\n",
        "# create the DataFrame\n",
        "df = pd.DataFrame(all_data, columns=['Product ID','Review'])\n",
        "## Group by product ID\n",
        "groupby_Id = df['Review'].groupby(df['Product ID'])\n",
        "## z = No. of products\n",
        "# z = len(list(groupby_Id))//100\n",
        "z = 1\n",
        "index = 28\n",
        "for j in range(z):\n",
        "  t = len(list(groupby_Id)[5][1])\n",
        "  # print(t)\n",
        "  for i in range(t):\n",
        "    ### Text Preprocessing of reviews for each Product\n",
        "    token_text = preprocess(list(groupby_Id)[5][1][index])\n",
        "    # print('Pos Tag:',token_text)\n",
        "    tag_list = []\n",
        "    new_tokens = []\n",
        "    for i in range(len(token_text)):\n",
        "        tag_list.append(token_text[i][1])\n",
        "    for i in range(len(tag_list)-1):\n",
        "#### Adj noun combinations\n",
        "       if ((tag_list[i] == 'JJ') and (tag_list[i+1] == 'NN' or tag_list[i] == 'NNP')):\n",
        "            new_tokens.append(token_text[i][0])\n",
        "            new_tokens.append(token_text[i+1][0])\n",
        "#### noun/pronoun verb noun combinations\n",
        "       if ((tag_list[i] == 'NN' or tag_list[i] == 'NNP') and (tag_list[i+1] == 'VB' or tag_list[i+1] == 'VBD' or  tag_list[i+1] == 'VBP')):\n",
        "             new_tokens.append(token_text[i][0])\n",
        "             new_tokens.append(token_text[i+1][0])\n",
        "### noun/noun combinations\n",
        "       if ((tag_list[i] == 'NN' or tag_list[i] == 'NNP') and (tag_list[i+1] == 'NN' or tag_list[i+1] == 'NNP')):\n",
        "            new_tokens.append(token_text[i][0])\n",
        "            new_tokens.append(token_text[i+1][0])\n",
        "    if(np.isin('\\n',new_tokens)):\n",
        "      new_tokens.remove('\\n')\n",
        "    new_tokens = np.unique(new_tokens)\n",
        "    print(\"New tokens: \",new_tokens)\n",
        "    print('\\n')   ##### Rule Based Aspects\n",
        "    index += 1 \n",
        "ifile.close()\n",
        "\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The Hosa XLR cables are affordable and very heavily made I have a large mixer and rack and cables everywhere I decided to purchase shorter cables and the Hosa cables 5ft measurement worked perfectly You really will not be disappointed with these ']\n",
            "New tokens:  ['cable' 'decide' 'disappoint' 'hosa' 'large' 'measurement' 'mixer'\n",
            " 'purchase' 'rack' 'short' 'work' 'xlr']\n",
            "\n",
            "\n",
            "['I bought these to go from my board to the amp  We use them for a mobile church so they take a beating  They are still going strong ']\n",
            "New tokens:  ['amp' 'board' 'church' 'mobile' 'strong' 'take' 'use']\n",
            "\n",
            "\n",
            "[\"Sturdy cord and plugs inexpensive good value  I don't require professional-level equipment so this cord serves my purposes well  Satisfied with purchase \"]\n",
            "New tokens:  ['cord' 'equipment' 'good' 'plug' 'professional-level' 'purchase'\n",
            " 'require' 'satisfy' 'serve' 'sturdy' 'value']\n",
            "\n",
            "\n",
            "['Use it every week at gigs   Solid no problems with the solder joints   A good quality cable at a very good price ']\n",
            "New tokens:  ['cable' 'gig' 'good' 'joint' 'price' 'problem' 'quality' 'solder' 'solid'\n",
            " 'week']\n",
            "\n",
            "\n",
            "[\"Hosa products are a good bang for the buck  I haven't looked up the specifications but I'm guessing the wire is 22 to 24 AWG but since it's only 10' long it's good enough \"]\n",
            "New tokens:  ['bang' 'buck' 'good' 'guess' 'hosa' \"i'm\" 'look' 'product'\n",
            " 'specification' 'wire']\n",
            "\n",
            "\n",
            "['This was exactly what I was after  I have a voice touch and needed a small cord to connect the mic to the voice touch and this was perfect  Before I used a 20 foot cord to go about 12 inches  I highly recommend this for those who keep the mic on a stand and have a voice touch or other stand connected device for vocals ']\n",
            "New tokens:  ['connect' 'cord' 'device' 'foot' 'go' 'mic' 'perfect' 'small' 'stand'\n",
            " 'touch' 'use' 'vocal' 'voice']\n",
            "\n",
            "\n",
            "[\"I bought these because I really had too long of mike cords for my solo live show  And these are really nice cords if you have a home portastudio recording studio like myself  Who needs all the spaghetti to trip on all over the place I bought two because I use a Digitech Live 2 Harmony processor and two XLR mike cords are required to make it operational  Good price here too  Hard to find just 10' mike cords  Usually longer ones are only obtainable \"]\n",
            "New tokens:  ['buy' 'cord' 'good' 'harmony' 'home' 'live' 'long' 'mike' 'nice'\n",
            " 'obtainable' 'place' 'portastudio' 'price' 'processor' 'record' 'require'\n",
            " 'solo' 'studio' 'trip' 'xlr']\n",
            "\n",
            "\n",
            "['This cable seems like it will last me for a while  As it is only being used to connect a DI box it will not get abused as much as the vocal mics always do ']\n",
            "New tokens:  ['box' 'cable' 'connect' 'di' 'get' 'last' 'seem' 'use']\n",
            "\n",
            "\n",
            "[\"These are not the greatest but they're cheap and they get to you fast when you need them  I've only had one fail and I've bought many of them to use in our broadcast studio Amazon is the first place I go when I need cables for audio video computers or concert lighting Good music to all\"]\n",
            "New tokens:  ['amazon' 'audio' 'broadcast' 'buy' 'cable' 'cheap' 'computer' 'concert'\n",
            " 'fail' 'first' 'get' 'go' 'good' 'great' \"i've\" 'light' 'many' 'music'\n",
            " 'place' 'studio' \"they're\" 'use' 'video']\n",
            "\n",
            "\n",
            "['This is a fine cable at a decent price point nothing exceptional mind but it gets the job done well enough ']\n",
            "New tokens:  ['cable' 'decent' 'exceptional' 'fine' 'get' 'job' 'mind' 'nothing'\n",
            " 'point' 'price']\n",
            "\n",
            "\n",
            "[\"I've used a lot of cables and I always come back to HOSA they are indeed some of the best audio cables in their price range on the market \"]\n",
            "New tokens:  ['audio' 'cable' \"i've\" 'lot' 'market' 'price' 'range' 'use']\n",
            "\n",
            "\n",
            "[\"I bought this cord after returning a cheap one that I should've known better than to buy  My son who has some experience as a musician recommended the Hosa brand when I was seeking a proper replacement I bought it for a small home recording setup which includes a Behringer C-1 Condenser Microphone and a Behringer Xenyx 302 Mixer and both ends make a reliable connection to both the board and the mic the sound quality is excellent An excellent value for the money \"]\n",
            "New tokens:  ['behringer' 'board' 'brand' 'buy' 'condenser' 'connection' 'cord' 'end'\n",
            " 'excellent' 'experience' 'home' 'hosa' 'include' 'make' 'mic'\n",
            " 'microphone' 'mixer' 'money' 'musician' 'proper' 'quality' 'recommend'\n",
            " 'record' 'reliable' 'replacement' 'return' 'seek' 'setup' 'small' 'son'\n",
            " 'sound' 'value' 'xenyx']\n",
            "\n",
            "\n",
            "['Nice solid cables with excellent support at the ends   Should last a lifetime of usage no problem and just what I needed to connect my tube preamp ']\n",
            "New tokens:  ['connect' 'end' 'excellent' 'lifetime' 'need' 'preamp' 'problem'\n",
            " 'support' 'tube' 'usage']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYIOnFTr-gs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}